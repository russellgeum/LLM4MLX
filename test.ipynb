{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "['model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.norm.weight']\n",
      "전체 키의 수:  164\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import mlx\n",
    "import mlx.nn as mx\n",
    "import mlx.core as mx_core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import safetensors\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from huggingface_hub import notebook_login\n",
    "from pprint import pprint\n",
    "from source.gemma.model import *\n",
    "from source.gemma.model_mlx import *\n",
    "\n",
    "# 테스트 모델 Gemma-2b-it 업로드\n",
    "path = \"./model/gemma-1.1-2b-it/\"\n",
    "model_name = \"model-{}-of-{}.safetensors\"\n",
    "model1 = safetensors.safe_open(path+model_name.format(\"00001\", \"00002\"), framework=\"pt\")\n",
    "model2 = safetensors.safe_open(path+model_name.format(\"00002\", \"00002\"), framework=\"pt\")\n",
    "\n",
    "print(model1.keys())\n",
    "print(model2.keys())\n",
    "\n",
    "# model1, model2의 tensor들을 답음\n",
    "tensors = {}\n",
    "for key in model1.keys():\n",
    "    tensors[key] = model1.get_tensor(key)\n",
    "for key in model2.keys():\n",
    "    tensors[key] = model2.get_tensor(key)\n",
    "print(\"전체 키의 수: \", len(tensors))\n",
    "\n",
    "\n",
    "# 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\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "input_text = \"Write me a poem about Machine Learning test.\"\n",
    "result = tokenizer(input_text, return_tensors=\"pt\")\n",
    "result = result[\"input_ids\"]\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 4, 4, 1])\n",
      "(1, 4, 2, 4, 2)\n"
     ]
    }
   ],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Precomputes the frequency cis.\n",
    "    \"\"\"\n",
    "    freqs = 1.0 / (theta**(torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device)\n",
    "    # 시간 벡터와 주파수 성분을 외저갛여 각 위치와 주파수 인덱스에 대한 2D 행렬 연산\n",
    "    freqs     = torch.outer(t, freqs).float()\n",
    "    # freqs 크기 1 텐서에 1 * cos(freqs) + 1 * sin(freqs)를 연산 -> complex64\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis\n",
    "\n",
    "\n",
    "def MLXprecompute_freqs_cis(dim: int, end: int, theta: float = 10000.0) -> mx_core.array:\n",
    "    \"\"\"\n",
    "    Precomputes the frequencey cis.\n",
    "    \"\"\"\n",
    "    freqs = 1.0 / (theta ** (mx_core.arange(0, dim, 2)[:(dim // 2)].astype(mx_core.float16) / dim))\n",
    "    t = mx_core.arange(end)\n",
    "    freqs = mx_core.outer(t, freqs)\n",
    "    cos = mx_core.ones_like(freqs) * mx_core.cos(freqs)\n",
    "    sin = mx_core.ones_like(freqs) * mx_core.sin(freqs)\n",
    "    freq_cis = mx_core.stack([cos, sin], axis = -1)\n",
    "    return freq_cis\n",
    "\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies the rotary embedding to the query and key tensors.\n",
    "    \"\"\"\n",
    "    # [B, C, H, W] -> [B, H, C, W] -> [B, H, C, W/2] & [B, H, C, W / 2]\n",
    "    # torch.view_as_complex with dim = -1를 통해 [B, H, C, W/2, 0]은 실수, [B, H, C, W/2, 1]은 허수 \n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis).type_as(x) ## 여기까지 결과가 같음\n",
    "\n",
    "    # 해결해야 할 부분\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "    return x_out\n",
    "\n",
    "\n",
    "def MLXapply_rotary_emb(x: mx_core.array, freqs_cis: mx_core.array) -> mx_core.array:\n",
    "    x_transpose = x.transpose(0, 2, 1, 3).astype(mx_core.float16)\n",
    "    x_real = x_transpose[:, :, :, :2]\n",
    "    x_imag = x_transpose[:, :, :, 2:]\n",
    "    x_ = mx_core.stack([x_real, x_imag], axis = -1)\n",
    "\n",
    "    x_out_real = x_[:, :, :, :, 0] * freqs_cis[:, :, 0] - x_[:, :, :, :, 1] * freqs_cis[:, :, 1]\n",
    "    x_out_imag = x_[:, :, :, :, 0] * freqs_cis[:, :, 1] + x_[:, :, :, :, 1] * freqs_cis[:, :, 0]\n",
    "    x_out = mx_core.stack([x_out_real, x_out_imag], axis = -1) ## 여기까지 결과가 같음\n",
    "\n",
    "    # 해결해야 할 부분\n",
    "    x_out = mx_core.stack([x_out[:, :, :, :, 0], x_out[:, :, :, :, 1]], axis = 2)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "dim = 4\n",
    "end = 4\n",
    "theta = 10000.0\n",
    "x = torch.randn(1, 1, 4, 4)\n",
    "freq1 = precompute_freqs_cis(dim, end, theta)\n",
    "freq2 = MLXprecompute_freqs_cis(dim, end, theta)\n",
    "\n",
    "\n",
    "outputs = apply_rotary_emb(x, freq1)\n",
    "MLXapply_rotary_emb(mx_core.array(x.numpy()), freq2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
