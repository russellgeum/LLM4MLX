{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "['model.layers.17.input_layernorm.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.norm.weight']\n",
      "model.embed_tokens.weight torch.Size([256000, 2048])\n",
      "model.layers.0.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.1.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.10.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.11.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.12.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.13.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.14.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.15.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.16.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.2.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.3.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.4.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.5.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.6.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.7.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.8.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.9.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([16384, 2048])\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([256, 2048])\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([2048, 2048])\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([256, 2048])\n",
      "model.layers.17.input_layernorm.weight torch.Size([2048])\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([2048, 16384])\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([2048])\n",
      "model.norm.weight torch.Size([2048])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import mlx\n",
    "import mlx.nn as mx\n",
    "import mlx.core as mx_core\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import safetensors\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from huggingface_hub import notebook_login\n",
    "from pprint import pprint\n",
    "\n",
    "# 테스트 모델 Gemma-2b-it 업로드\n",
    "path = \"./model/Gemma-2b-it/\"\n",
    "model_name = \"model-{}-of-{}.safetensors\"\n",
    "model1 = safetensors.safe_open(path+model_name.format(\"00001\", \"00002\"), framework=\"pt\")\n",
    "model2 = safetensors.safe_open(path+model_name.format(\"00002\", \"00002\"), framework=\"pt\")\n",
    "\n",
    "print(model1.keys())\n",
    "print(model2.keys())\n",
    "\n",
    "# model1, model2의 tensor들을 답음\n",
    "tensors = {}\n",
    "for key in model1.keys():\n",
    "    tensors[key] = model1.get_tensor(key)\n",
    "for key in model2.keys():\n",
    "    tensors[key] = model2.get_tensor(key)\n",
    "for key in tensors:\n",
    "    print(key, tensors[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0233,  0.9686, -0.3742,  ..., -0.6960, -0.3375, -0.1172],\n",
      "        [-0.7276, -0.7022, -1.0547,  ...,  0.3612, -0.3441,  0.3949],\n",
      "        [-0.8054,  0.3337,  0.1827,  ..., -0.0143,  0.4280,  0.4908],\n",
      "        ...,\n",
      "        [ 0.2764, -0.6819, -1.0033,  ..., -0.2844, -0.4962, -0.8181],\n",
      "        [ 0.7598, -0.0205, -0.2107,  ...,  0.1204, -1.0318,  0.1993],\n",
      "        [-0.1827, -0.6451, -0.1667,  ..., -0.1079, -0.4077, -1.1762]])\n",
      "array([[1.02331, 0.968554, -0.374168, ..., -0.695993, -0.33745, -0.117239],\n",
      "       [-0.727575, -0.702247, -1.0547, ..., 0.361208, -0.344141, 0.394925],\n",
      "       [-0.805406, 0.333706, 0.182659, ..., -0.0142567, 0.427993, 0.490802],\n",
      "       ...,\n",
      "       [0.276407, -0.681926, -1.00328, ..., -0.284429, -0.496206, -0.818095],\n",
      "       [0.759834, -0.0204772, -0.210725, ..., 0.120362, -1.03176, 0.199336],\n",
      "       [-0.182736, -0.645108, -0.166685, ..., -0.107906, -0.407675, -1.17617]], dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# torch와 mlx 간의 weight가 서로 다름\n",
    "inputs = torch.randn(200, 100)\n",
    "layer1 = nn.Linear(100, 200, dtype=torch.float32).requires_grad_(False)\n",
    "layer2 = mx.Linear(100, 200)\n",
    "output1 = layer1(inputs)\n",
    "layer2.weight = mx_core.array(layer1.weight.numpy())\n",
    "layer2.bias   = mx_core.array(layer1.bias.numpy())\n",
    "output2 = layer2(mlx.core.array(inputs))\n",
    "print(output1)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 2048])\n",
      "torch.Size([2048, 2048])\n",
      "torch.Size([2048, 2048]) torch.Size([16384, 2048])\n"
     ]
    }
   ],
   "source": [
    "hidden_size=2048\n",
    "intermediate_size=16384\n",
    "inputs = torch.randn(hidden_size, intermediate_size)\n",
    "weight1 = nn.Parameter(torch.empty(hidden_size, intermediate_size))\n",
    "weight2 = nn.Parameter(torch.empty(hidden_size, intermediate_size))\n",
    "weight3 = nn.Parameter(torch.empty(intermediate_size, hidden_size))\n",
    "\n",
    "gate = F.linear(inputs, weight1)\n",
    "print(gate.shape)\n",
    "up   = F.linear(inputs, weight2)\n",
    "print(up.shape)\n",
    "fuse = gate * up\n",
    "print(fuse.shape, weight3.shape)\n",
    "down = F.linear(fuse, weight3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
